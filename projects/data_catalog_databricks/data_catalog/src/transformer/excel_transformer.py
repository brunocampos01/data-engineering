from typing import List

from pyspark.sql import (
    DataFrame,
    SparkSession,
)
from pyspark.sql.functions import (
    col,
    when,
)

from data_catalog.src.transformer.base_transformer import BaseTransformer

class ExcelTransformer(BaseTransformer):
    """A transformer for generating Excel file updated."""
    def __init__(self, spark: SparkSession, layer_name: str):
        super().__init__(spark, layer_name)

    @staticmethod
    def __get_manual_input_from_ui(df: DataFrame, type_table: str) -> DataFrame:
        """
        Read the descriptions inputted from Unity Catalog interface (UI).
        If the user to accept some AI description suggestion and the excel description is null
        this function will update the excel keeping the manual input.
        
        Args:
            df (DataFrame): The DataFrame with uc_table_description and table_description cols.

        Returns:
            DataFrame: The resulting DataFrame after processing.
        """
        return df \
            .withColumn(f'{type_table}_description', when(
                col(f'{type_table}_description').isNull(), col(f'uc_{type_table}_description')
                ).otherwise(col(f'{type_table}_description'))
            ) \
            .drop(f'uc_{type_table}_description')

    @staticmethod
    def __join(
        df_uc: DataFrame, df_landing: DataFrame, join_cond: List, table_name: str
    ) -> DataFrame:
        """
        This function join df from landing witch contains coments in views
        with df from Unity Catalog.
        After join, sort the df to become more organize the Excel file.
        
        Args:
            df_uc (DataFrame): The DataFrame with Unity Catalog metadata
            df_landing (DataFrame): The DataFrame generated in metadata_uc_update_job
            join_cond (List): The list df's columns to make the join
                e.g.:
                    df_uc['layer'] ==      df_landing['layer'],
                    df_uc['source_raw'] == df_landing['source_raw']
            table_name (str): The name of that contains the metadata. e.g.: field

        Returns:
            DataFrame: The resulting DataFrame with 
                       all uc columns and the description form landing df.
        """
        return df_uc \
            .join(df_landing, on=join_cond, how='left') \
            .select(df_uc['*'], df_landing[f'{table_name}_description']) \
            .orderBy('layer', 'source_raw', 'source', 'table')

    def __process_data_stewards(self):
        """
        Process data stewards from the 'data_stewards' table.

        Returns:
            DataFrame: A df containing data stewards, ordered by 'data_steward'.
        """
        return self.spark.sql("""
            SELECT * FROM `data_stewards` 
            ORDER BY `data_steward`
        """)

    def __process_layers(self):
        """
        Process layers from the 'layers' table.

        Returns:
            DataFrame: A df containing layers, ordered by 'layer'.
        """
        return self.spark.sql("""
            SELECT * FROM `layers` 
            ORDER BY `layer`
        """)

    def __process_sources(self):
        """
        Process sources from the 'sources' table and clean column names.

        Returns:
            DataFrame: A df containing cleaned source data.
        """
        df = self.spark.sql("""
            SELECT * FROM `sources` 
            ORDER BY `source`
        """)
        list_to_exclude = ['source_created_in_uc_at', 'source_last_updated_at']
        return self.clean_col_name(
            df=df, 
            table_name='source', 
            list_to_exclude=list_to_exclude,
        )

    def __process_tables(
        self, place: str = None, df_uc: DataFrame = None, df_landing: DataFrame = None
        ) -> DataFrame:
        """
        Process tables based on place.
        
        Args:
            place (str): The place to process tables from. Can be 'uc' or 'landing'.
            df_uc (DataFrame): df representing tables from unity catalog.
            df_landing (DataFrame): df representing tables from landing generated by metadata_uc_update_job.

        Returns:
            DataFrame: Processed DataFrame containing tables metadata.
        """
        if place != None:
            if place == 'uc':
                df = self.spark.sql("""
                    SELECT * FROM `tables` 
                    ORDER BY `layer`, `source_raw`, `source`, `table`
                """)
                df = df.withColumnRenamed('table_description', 'uc_table_description')
            
            elif place == 'landing':
                df = self._load_delta_table('tables_only_views', 'landing', 'data_catalog_processed')

            list_to_exclude = ['table_created_in_uc_at', 'table_last_updated_at', 
                               'table_last_data_updated_at', 'table_last_schema_updated_at' ,
                               'tag_table_last_data_steward', 'layer_raw', 
                               'array_tag_table_source_system', 'table_id']
            return self.clean_col_name(
                df=df, 
                table_name='table', 
                list_to_exclude=list_to_exclude,
            )
        else:
            join_cond = [
                df_uc['layer'] ==      df_landing['layer'],
                df_uc['source_raw'] == df_landing['source_raw'],
                df_uc['table'] ==      df_landing['table'],
            ]
            df = self.__join(df_uc, df_landing, join_cond, 'table')
            return self.__get_manual_input_from_ui(df, 'table')


    def __process_fields(
        self, place: str = None, df_uc: DataFrame = None, df_landing: DataFrame = None
    ) -> DataFrame:
        """
        Process fields based on the place provided.

        Args:
            place (str): Specifies the source of data ('uc' or 'landing').
            df_uc (DataFrame): df containing fields from uc.
            df_landing (DataFrame): df representing tables from landing generated by metadata_uc_update_job.

        Returns:
            DataFrame: Processed df containing fields.
        """
        if place != None:
            if place == 'uc':
                df = self.spark.sql("""
                    SELECT * FROM `fields` 
                    ORDER BY `layer`, `source_raw`, `source`, `table`, `field`
                """)
                df = df.withColumnRenamed('field_description', 'uc_field_description')
                df = df.drop('field_description')

            elif place == 'landing':
                df = self._load_delta_table('fields_only_views', 'landing', 'data_catalog_processed')
                
            list_to_exclude = ['layer_raw', 'table_id', 'field_id', 
                               'field_created_in_uc_at', 'field_last_updated_at',
                               'array_tag_field_data_usage']
            df = self.clean_col_name(
                df=df, 
                table_name='field', 
                list_to_exclude=list_to_exclude,
            )

            return df

        else:
            join_cond = [
                df_uc['layer'] ==      df_landing['layer'],
                df_uc['source_raw'] == df_landing['source_raw'],
                df_uc['table'] ==      df_landing['table'],
                df_uc['field'] ==      df_landing['field']
            ]
            df = self.__join(df_uc, df_landing, join_cond, 'field')
            return self.__get_manual_input_from_ui(df, 'field')

    def execute(
        self,
        table_name: str,
        place: str = None,
        df_uc: DataFrame = None,
        df_landing: DataFrame = None,
    ) -> DataFrame:
        df = self._load_delta_table(table_name, 'silver')
        df.createOrReplaceTempView(table_name)

        if table_name == 'sources':
            return self.__process_sources()

        elif table_name == 'tables':
            return self.__process_tables(place, df_uc, df_landing)

        elif table_name == 'fields':
            return self.__process_fields(place, df_uc, df_landing)
        
        elif table_name == 'layers':
            return self.__process_layers()

        elif table_name == 'data_stewards':
            return self.__process_data_stewards()

        else:
            raise ValueError(f"Unsupported table_name: {table_name}")
